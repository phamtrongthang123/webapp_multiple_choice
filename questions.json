[
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "In the ML System Design interview delivery framework, what is the recommended order of steps?",
    "options": {
      "A": "Modeling → Data → Problem Framing → Evaluation",
      "B": "Problem Framing → High-level Design → Data and Features → Modeling → Inference and Evaluation",
      "C": "Data Collection → Feature Engineering → Model Training → Deployment",
      "D": "Business Requirements → Technical Implementation → Testing → Launch"
    },
    "answer": "B",
    "explanation": "The recommended framework follows: Problem Framing (5-7 min) → High-level Design (2-3 min) → Data and Features (10 min) → Modeling (10 min) → Inference and Evaluation (7 min). This structure ensures you cover all important aspects while demonstrating systematic thinking."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "During the Problem Framing phase, what three things should you accomplish?",
    "options": {
      "A": "Define metrics, choose model architecture, select features",
      "B": "Clarify the problem, establish business objective, decide on ML objective",
      "C": "Gather data, clean data, split data",
      "D": "Design API, choose database, set up infrastructure"
    },
    "answer": "B",
    "explanation": "Problem Framing involves: 1) Clarifying the problem and constraints through targeted questions, 2) Establishing a clear business objective that the ML system will help achieve, and 3) Translating that into a concrete ML objective you can build around."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "Why is 'maximize model accuracy' often a poor business objective?",
    "options": {
      "A": "It's too easy to achieve",
      "B": "Accuracy doesn't reflect what users/business actually care about and can be misleading with class imbalance",
      "C": "It requires too much data",
      "D": "It's only useful for regression problems"
    },
    "answer": "B",
    "explanation": "End-users and business don't care about model accuracy - they care about outcomes like user experience or cost reduction. Accuracy is also a poor proxy for harm in imbalanced datasets, where a model could achieve 99% accuracy by always predicting the majority class."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What is the key difference between a business objective and an ML objective?",
    "options": {
      "A": "Business objectives are about profit, ML objectives are about data",
      "B": "Business objectives are what success looks like for the organization, ML objectives translate that into trainable targets",
      "C": "Business objectives are long-term, ML objectives are short-term",
      "D": "There is no difference; they are the same thing"
    },
    "answer": "B",
    "explanation": "The business objective represents the end-goal the business cares about (e.g., reducing costs, improving retention). The ML objective translates this into something you can optimize (e.g., a classification or ranking task with specific metrics). They're related but distinct."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "In the Data and Features discussion, what three categories of training data should you consider?",
    "options": {
      "A": "Text, images, and video",
      "B": "Supervised, semi-supervised, and self-supervised/unsupervised",
      "C": "Training, validation, and test",
      "D": "Batch, streaming, and real-time"
    },
    "answer": "B",
    "explanation": "Great solutions often leverage: 1) Supervised data with explicit labels, 2) Semi-supervised data like user reports or weak labels, and 3) Self-supervised data like predicting comments from posts. The latter categories often have orders of magnitude more data available."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What is 'earmarking' in the context of an ML design interview?",
    "options": {
      "A": "A technique for labeling data",
      "B": "Making a verbal note to your interviewer about a topic you'll cover later",
      "C": "A method for feature selection",
      "D": "A way to prioritize model architectures"
    },
    "answer": "B",
    "explanation": "Earmarking is verbally noting to your interviewer that you'd expect to cover a specific topic later. It prevents a red-flag from being raised that you 'missed it' while avoiding burning time in the moment. If the interviewer thinks it's important, they'll probe with questions."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "Why should you establish benchmark models before diving into complex architectures?",
    "options": {
      "A": "To show you know multiple techniques",
      "B": "Because simple models are always better",
      "C": "To provide a baseline for comparison and demonstrate engineering maturity",
      "D": "Because interviewers always ask about logistic regression"
    },
    "answer": "C",
    "explanation": "Starting with benchmark models: 1) Provides a yardstick to evaluate added complexity, 2) Shows engineering maturity, 3) Moves the conversation from theoretical to practical tradeoffs, and 4) Acts as a gap-filler if you need multiple models. Sometimes simple is all you need!"
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What is a common mistake candidates make when discussing features?",
    "options": {
      "A": "Not mentioning enough features",
      "B": "Dumping a laundry list of features without considering impact or practicality",
      "C": "Only discussing numerical features",
      "D": "Spending too little time on features"
    },
    "answer": "B",
    "explanation": "Feature discussions are the biggest tarpit for candidates. Rather than rattling off as many features as possible, senior candidates demonstrate they can generate hypotheses about what data is useful and prioritize evaluation. It's more important to justify choices than be exhaustive."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "When would a 'classical' ML model like logistic regression or GBDT be preferable to deep learning?",
    "options": {
      "A": "Never - deep learning is always better",
      "B": "When you have limited data only",
      "C": "When compute constraints are high, latency is critical, or the performance gap doesn't justify complexity",
      "D": "Only for regression problems"
    },
    "answer": "C",
    "explanation": "Classical ML approaches can still be competitive in environments with heavy compute pressure, strict latency requirements, or when the performance improvement from deep learning doesn't justify the added complexity. Demonstrating this knowledge shows practical experience."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What should you avoid doing with the whiteboard diagram during an ML design interview?",
    "options": {
      "A": "Drawing boxes and arrows",
      "B": "Over-optimizing the diagram at the expense of mental bandwidth and interviewer's time",
      "C": "Showing the high-level architecture",
      "D": "Communicating how pieces fit together"
    },
    "answer": "B",
    "explanation": "Some candidates spend too much time perfecting diagrams, which distracts from the real signal interviewers look for. The whiteboard is a communication aid, not a final product. Your job is to get on the same page and minimize miscommunication, not to create perfect drawings."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is the recommended structure for approaching evaluation in ML system design?",
    "options": {
      "A": "Online metrics → Offline metrics → A/B testing",
      "B": "Business Objective → Product Metrics → ML Metrics → Evaluation Methodology → Address Challenges",
      "C": "Precision → Recall → F1 Score → ROC-AUC",
      "D": "Train metrics → Validation metrics → Test metrics"
    },
    "answer": "B",
    "explanation": "The recommended evaluation structure: 1) Start with business objective, 2) Define product metrics that indicate user-facing success, 3) Detail ML metrics aligned with product goals, 4) Outline online/offline evaluation approaches, 5) Address potential challenges like imbalanced data or fairness."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "For classification problems with strong class imbalance (e.g., 99% negative, 1% positive), which metric is preferred over ROC-AUC?",
    "options": {
      "A": "Accuracy",
      "B": "F1 Score",
      "C": "PR-AUC (Precision-Recall AUC)",
      "D": "Mean Squared Error"
    },
    "answer": "C",
    "explanation": "PR-AUC is far better than ROC-AUC for highly imbalanced problems. ROC-AUC can look fantastic even if the classifier is completely useless for imbalanced data because it considers true negatives, which dominate in imbalanced datasets."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is the main advantage of interleaving tests over traditional A/B tests for recommender systems?",
    "options": {
      "A": "They're cheaper to implement",
      "B": "They require 10-20x less traffic to detect the same effect size",
      "C": "They don't require randomization",
      "D": "They always show larger effects"
    },
    "answer": "B",
    "explanation": "A/B tests assign different users to different rankers (unpaired comparison). Interleaving shows one mixed list to the same user (paired test). Because every click simultaneously provides evidence for one ranker and against another, variance drops sharply - typically 10-20x less traffic needed."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is the 'feedback loop' problem in ML evaluation?",
    "options": {
      "A": "Users providing too much feedback",
      "B": "The model's predictions influence future training data, potentially amplifying biases",
      "C": "Feedback arriving too slowly",
      "D": "Negative feedback being more common than positive"
    },
    "answer": "B",
    "explanation": "Feedback loops occur when model predictions influence future data, potentially amplifying biases or errors over time. Solutions include: injecting randomness, maintaining golden sets unaffected by the model, counterfactual logging, and careful exploration strategies."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "For recommender systems, why might optimizing for short-term metrics cannibalize long-term results?",
    "options": {
      "A": "Short-term metrics are always wrong",
      "B": "Optimizing for clicks might promote addictive content that harms retention and satisfaction",
      "C": "Long-term metrics don't exist",
      "D": "Short-term experiments are invalid"
    },
    "answer": "B",
    "explanation": "Short-term metrics like CTR might promote clickbait or addictive content that users click but later regret, harming retention. Product metrics should accumulate over sessions, and experiments must be long enough to capture long-term effects."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What does NDCG@k measure in search and recommendation systems?",
    "options": {
      "A": "The number of clicks in top k results",
      "B": "Relevance with position-based discounting - higher ranked relevant items contribute more",
      "C": "The total number of relevant documents",
      "D": "User satisfaction score"
    },
    "answer": "B",
    "explanation": "NDCG (Normalized Discounted Cumulative Gain) discounts relevance by log-rank position. It's a good all-rounder metric that weights high-rank relevance more heavily, reflecting that users care more about top results being relevant."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is 'presentation bias' in search evaluation and how can it be addressed?",
    "options": {
      "A": "Users prefer visually appealing results; use better CSS",
      "B": "Click data is biased by previous rankings; use inverse-propensity weighting or interleaving",
      "C": "Some results load faster; optimize latency",
      "D": "Mobile users see different results; test both platforms"
    },
    "answer": "B",
    "explanation": "Click logs are biased by previous rankings - users click more on top results regardless of relevance. If you build labels from clicks, you need debiasing techniques like inverse-propensity weighting or deterministic interleaving. Interviewers often probe this point."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "For classification systems, what is the purpose of 'shadow mode' testing?",
    "options": {
      "A": "Testing models in production without affecting users",
      "B": "Running the model on synthetic data",
      "C": "Testing only during off-peak hours",
      "D": "Using a smaller version of the model"
    },
    "answer": "A",
    "explanation": "In shadow mode, the model makes predictions but doesn't take action, allowing reviewers to validate results without affecting users. Once confidence is established, you can graduate to A/B testing that measures both technical metrics and business impacts."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "Why is 'estimating prevalence' challenging for many classification problems?",
    "options": {
      "A": "Prevalence never changes",
      "B": "True prevalence requires expensive resources to measure, and random sampling has high variance for rare events",
      "C": "Models automatically output prevalence",
      "D": "Prevalence is not important"
    },
    "answer": "B",
    "explanation": "True prevalence often requires expensive manual review to determine. Random sampling for rare events (e.g., <1% positive) has high variance - sampling 100 examples might yield only 1 positive, making prevalence estimates unreliable."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is 'label efficiency' and why is it important in classification systems?",
    "options": {
      "A": "How quickly labels can be stored in a database",
      "B": "Obtaining high-quality labels efficiently, using techniques like stratified sampling or active learning",
      "C": "How many labels can fit in memory",
      "D": "The compression ratio of label files"
    },
    "answer": "B",
    "explanation": "Obtaining high-quality labels is expensive, especially for specialized domains. Random sampling is inefficient for imbalanced problems. Stratified sampling using classifier scores or active learning to prioritize informative examples helps maximize label utility."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why is 'maximize the amount of harmful content removed' a poor business objective for content moderation?",
    "options": {
      "A": "It's hard to measure",
      "B": "It incentivizes flagging all content as harmful, leading to excessive false positives",
      "C": "Harmful content is rare",
      "D": "It requires too much compute"
    },
    "answer": "B",
    "explanation": "This objective creates perverse incentives to flag as much content as possible regardless of actual harm. With classification, there's always a tradeoff between false positives and false negatives. This objective ignores that balance and would frustrate legitimate users."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What is a better business objective for content moderation than 'maximize removed content'?",
    "options": {
      "A": "Maximize model accuracy",
      "B": "Minimize the number of views of harmful content, subject to precision guardrails",
      "C": "Remove all content with profanity",
      "D": "Maximize human reviewer workload"
    },
    "answer": "B",
    "explanation": "Minimizing views focuses on actual harm caused - exposure to harmful content. The precision guardrails ensure legitimate content isn't over-restricted. This naturally prioritizes catching widely-viewed harmful content and viral content early."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why is focusing on 'views' rather than 'posts' important for content moderation?",
    "options": {
      "A": "Views are easier to count",
      "B": "Posts with many views cause more harm, and posts with no views cause no harm regardless of content",
      "C": "Users don't see posts",
      "D": "Views are the same as posts"
    },
    "answer": "B",
    "explanation": "A harmful post that gets no views causes no user harm. Posts that get many views (especially viral content) are infinitely more important. This insight also means harmful content becomes easier to detect over time as user behaviors provide additional signals."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What is the advantage of using user reports as a training signal for harmful content detection?",
    "options": {
      "A": "They're always accurate",
      "B": "They provide scalable semi-supervised data that's orders of magnitude larger than expert labels",
      "C": "They're free from bias",
      "D": "They replace the need for any other labels"
    },
    "answer": "B",
    "explanation": "User reports provide a semi-supervised signal that's likely correlated with the true label. You might have 10M reports vs 50k labeled examples. While noisier than expert labels (can be adversarial or incorrect), their volume makes them invaluable for training."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "How can user comments on posts be leveraged for content moderation training?",
    "options": {
      "A": "Use them as direct labels",
      "B": "Train a model to predict comments from post body, learning rich semantic representations",
      "C": "Delete all posts with comments",
      "D": "Only moderate posts with comments"
    },
    "answer": "B",
    "explanation": "Comments like 'Gross!' or 'This is disgusting' correlate with harmful content. A model trained to predict comments from posts learns useful representations - this self-supervised approach provides orders of magnitude more training signal than labeled data alone."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What is a 'late fusion' architecture in multi-modal classification?",
    "options": {
      "A": "Processing text and images together from the start",
      "B": "Processing text and images through separate encoders, then concatenating their embeddings before classification",
      "C": "Only using text features",
      "D": "Using fusion reactors for processing"
    },
    "answer": "B",
    "explanation": "Late fusion processes text and images through separate encoders, then concatenates their embeddings along with other features before passing through a classification head. It allows some cross-modal interaction while leveraging pre-trained models efficiently."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why might a 'lightweight update network' be useful in content moderation?",
    "options": {
      "A": "To reduce model size",
      "B": "To efficiently incorporate new behavioral signals without re-running the heavy content encoder",
      "C": "To make the model faster",
      "D": "To compress images"
    },
    "answer": "B",
    "explanation": "Content features (text, image) can be computed once when posted, but behavioral features (reactions, reports) change over time. A lightweight update network can merge new behavioral inputs with cached content embeddings efficiently without re-running expensive encoders."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "For handling class imbalance in harmful content detection, what combination of techniques is commonly used?",
    "options": {
      "A": "Only collect more harmful examples",
      "B": "Balanced sampling during training and loss weighting to fine-tune precision-recall tradeoff",
      "C": "Ignore the imbalance",
      "D": "Use accuracy as the only metric"
    },
    "answer": "B",
    "explanation": "With harmful content being <1% of posts, a naive model would predict 'not harmful' for everything and achieve 99% accuracy. Balanced sampling ensures the model sees enough positive examples, while loss weighting helps tune the precision-recall tradeoff."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What behavioral features can indicate harmful content after a post is published?",
    "options": {
      "A": "The color of the post background",
      "B": "Negative reactions, hides, reports, and negative comment sentiment",
      "C": "The time zone of the poster",
      "D": "The file size of attached media"
    },
    "answer": "B",
    "explanation": "Behavioral signals like negative reactions (angry, hide), reports, shares-per-view ratio, and comments with negative sentiment (e.g., 'Gross!', 'This is disgusting') provide strong signals about harmful content that emerge after posting."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why might content detection get easier over time after a post is published?",
    "options": {
      "A": "The model trains while the post exists",
      "B": "User behaviors (blocks, reports, negative reactions) provide additional signals that reduce classification difficulty",
      "C": "Content naturally becomes less harmful",
      "D": "More computing resources become available"
    },
    "answer": "B",
    "explanation": "Harmful content becomes easier to detect with time because user behaviors (blocking, unliking, reporting, commenting negatively) provide additional signals. This insight allows us to trade off detection speed vs. accuracy based on our business objective."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why is bot detection considered an 'adversarial' ML problem?",
    "options": {
      "A": "Users are hostile to bot detection",
      "B": "Bot authors constantly adapt their tactics to evade detection systems",
      "C": "Bots attack the detection system directly",
      "D": "Detection systems compete with each other"
    },
    "answer": "B",
    "explanation": "Bot detection is adversarial because bot authors constantly adapt to evade detection. Patterns that work today might fail tomorrow. Early bots had obvious tells like spamming at superhuman frequencies, but modern bots are much more sophisticated."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why is 'maximize bot detection rate' a better objective than 'maximize detected bots'?",
    "options": {
      "A": "It sounds more professional",
      "B": "Adding false positive constraints protects legitimate users while still enabling aggressive detection",
      "C": "It requires less computation",
      "D": "Bots prefer this metric"
    },
    "answer": "B",
    "explanation": "Maximizing detected bots creates incentives to be overly aggressive, catching legitimate users in the crossfire. Adding false positive rate constraints (e.g., <1%) protects user trust while still allowing aggressive bot detection within those bounds."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What is the challenge with ground truth labels in bot detection?",
    "options": {
      "A": "There are too many labels",
      "B": "Manual investigation is expensive and investigators can only verify hundreds of accounts per week",
      "C": "Labels are always wrong",
      "D": "Labels expire immediately"
    },
    "answer": "B",
    "explanation": "Ground truth from investigators is the highest quality data but extremely limited - they can only investigate low hundreds of accounts per week. This scarcity means you must be strategic about which accounts to send for review."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "How can network-based features help identify bot accounts?",
    "options": {
      "A": "By measuring internet speed",
      "B": "By identifying accounts with similar IP addresses, behavior patterns, or registration patterns as known bots",
      "C": "By checking the router configuration",
      "D": "By measuring packet loss"
    },
    "answer": "B",
    "explanation": "The social graph helps propagate labels: IP address clustering (accounts from malicious IPs), behavioral similarity to confirmed bots, and bulk registration patterns can identify entire bot networks rather than individual accounts."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What type of synthetic data generation can help with class imbalance in bot detection?",
    "options": {
      "A": "Random noise injection",
      "B": "Conditional GANs that generate realistic bot behavior patterns",
      "C": "Simple duplication of existing examples",
      "D": "Image augmentation"
    },
    "answer": "B",
    "explanation": "Conditional GANs (like CALEB) can generate realistic bot behavior patterns across multiple dimensions: temporal activity, content generation styles, and network formation behaviors. This helps models learn sophisticated patterns underrepresented in real data."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What is the 'survival bias' challenge in bot detection training data?",
    "options": {
      "A": "Bots live longer than users",
      "B": "As detection improves, only sophisticated bots remain, making training data naturally shift to harder examples",
      "C": "Training data survives longer in storage",
      "D": "Models survive longer with better data"
    },
    "answer": "B",
    "explanation": "The adversarial nature creates survival bias: as detection improves, simple bots are filtered out, so observed bots become more sophisticated. This means training data naturally shifts toward harder examples - both a blessing (harder training) and curse (distribution shift)."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why might a GRU be preferred over a transformer for bot detection sequence modeling?",
    "options": {
      "A": "GRUs are always better",
      "B": "We're fitting to sketchy behaviors rather than nuanced language understanding, so simpler is sufficient",
      "C": "Transformers don't work on sequences",
      "D": "GRUs are more expensive"
    },
    "answer": "B",
    "explanation": "For bot detection, we're identifying suspicious behavioral patterns, not understanding nuanced language. A GRU is simpler, faster, and sufficient for capturing temporal patterns in user actions like post frequency, login patterns, and device switches."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What is the purpose of using GraphSAGE for bot detection?",
    "options": {
      "A": "To generate graphs",
      "B": "It's inductive (handles new nodes without retraining) and can model social network relationships",
      "C": "To compress graph data",
      "D": "To visualize networks"
    },
    "answer": "B",
    "explanation": "GraphSAGE is inductive, meaning it can embed new accounts without retraining the entire model. It also treats different edge types differently (follow vs. reply) which helps distinguish spam accounts that auto-follow thousands from well-connected legitimate users."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "How does pretraining help bot detection models with limited labeled data?",
    "options": {
      "A": "It doesn't help",
      "B": "Self-supervised tasks (predicting masked features, next events) help the model learn the 'language' of normal behavior first",
      "C": "It makes labeling faster",
      "D": "It creates more labeled data"
    },
    "answer": "B",
    "explanation": "With limited labels and a large model, pretraining is essential. The graph branch learns by predicting masked node attributes and identifying real vs. fake neighbor pairs. The sequence branch predicts masked/next events. This captures normal patterns before fine-tuning."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why is 'defense in depth' important for bot detection systems?",
    "options": {
      "A": "To use more models",
      "B": "Multiple layers provide safety nets since each stage can have gaps or be evaded differently",
      "C": "To increase latency",
      "D": "To make the system more complex"
    },
    "answer": "B",
    "explanation": "Production safety/integrity systems are often built in layers responding to real attacks. Additional layers provide safety nets - simple heuristics catch obvious bots, supervised models catch known patterns, and unsupervised approaches can identify novel bot behaviors."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What actions can be taken besides banning when a bot is detected?",
    "options": {
      "A": "Only banning is possible",
      "B": "Demotions (reducing visibility) and rate limits can minimize damage while reducing false positive impact",
      "C": "Notify the bot operator",
      "D": "Increase the account's privileges"
    },
    "answer": "B",
    "explanation": "Since the objective is minimizing impact of bot activity, you can: ban when confident, demote (reduce visibility) when unsure, or rate-limit interactions. This minimizes collateral damage from false positives while still reducing bot impact."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why is 'maximize click-through rate (CTR)' a poor business objective for video recommendations?",
    "options": {
      "A": "CTR is hard to measure",
      "B": "It leads to clickbait optimization - users may click on misleading thumbnails but not retain",
      "C": "CTR is always low",
      "D": "Videos don't have CTR"
    },
    "answer": "B",
    "explanation": "CTR optimization can lead to clickbait thumbnails and misleading titles. Users might click but if they don't retain (watch or come back), the platform loses. CTR ignores the quality of engagement that follows the click."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is a better business objective than pure watch time for video recommendations?",
    "options": {
      "A": "Maximize video length",
      "B": "Maximize quality-adjusted watch time, combining watch time with quality signals like ratings and completion",
      "C": "Minimize buffering time",
      "D": "Maximize upload frequency"
    },
    "answer": "B",
    "explanation": "Pure watch time can promote addictive but low-quality content and bias toward longer videos. Quality-adjusted watch time combines watch time with signals like ratings, completion rates, and sharing behavior to optimize for 'time well spent.'"
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is the purpose of the multi-stage architecture in recommendation systems?",
    "options": {
      "A": "To make the system more complex",
      "B": "To efficiently filter billions of candidates down to the best few through progressively more expensive ranking",
      "C": "To increase latency",
      "D": "To use more servers"
    },
    "answer": "B",
    "explanation": "With billions of videos and users, you can't run expensive models on everything. Candidate generators produce O(10k) candidates, a light ranker filters to O(100) optimizing for recall, then a heavy ranker scores the final set for precision."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why might candidate generators with limited context be highly cacheable?",
    "options": {
      "A": "They use less memory",
      "B": "Universal generators (like 'top 10k videos') can be pre-computed and reused across users",
      "C": "They're faster to compute",
      "D": "They don't need databases"
    },
    "answer": "B",
    "explanation": "Candidate generators with limited context - like 'top 10k platform videos' - don't change per user and can be pre-computed and cached. This is a key ingredient for scaling to billions of users and requests."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is the key priority for the light ranker in recommendation systems?",
    "options": {
      "A": "High precision",
      "B": "High recall - don't discard potentially great recommendations",
      "C": "High latency",
      "D": "Complex architecture"
    },
    "answer": "B",
    "explanation": "The light ranker's job is to filter candidates while optimizing for recall - ensuring we don't discard videos that might end up being great recommendations. Precision is the heavy ranker's job. Missing a great video at this stage can't be recovered."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why might a tree-based model (GBDT) be preferred for the light ranker?",
    "options": {
      "A": "Trees are more accurate",
      "B": "They run on CPU with sub-millisecond latency, enabling processing of billions of candidates economically",
      "C": "Trees are more interpretable",
      "D": "GPUs can't handle ranking"
    },
    "answer": "B",
    "explanation": "With heavy compute pressure and lower precision requirements, GBDTs (XGBoost, LightGBM) can run on economical CPUs with sub-millisecond latency. This allows churning through billions of candidates. GPUs are saved for the heavy ranker."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is a 'two-tower' architecture and when is it used?",
    "options": {
      "A": "An architecture with two data centers",
      "B": "Parallel towers for user and item embeddings, trained with triplet loss for efficient retrieval",
      "C": "An architecture with two databases",
      "D": "Two separate models for different users"
    },
    "answer": "B",
    "explanation": "Two-tower architectures train parallel towers to produce embeddings for users and items using triplet loss. This enables efficient retrieval via vector similarity search. Common for candidate generation where you need fast approximate matching."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is the purpose of multi-task learning in the heavy ranker?",
    "options": {
      "A": "To increase model size",
      "B": "Multiple prediction heads (watch time, clicks, likes, shares) act as regularizers and provide richer signals",
      "C": "To slow down training",
      "D": "To reduce accuracy"
    },
    "answer": "B",
    "explanation": "Predicting multiple outcomes (watch time, CTR, like probability, share probability, completion rate) provides: regularization against overfitting to any single metric, auxiliary training signals, better cold-start handling, and inputs to the value model."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why is DLRM (Deep Learning Recommendation Model) better than a simple MLP for recommendations?",
    "options": {
      "A": "It's always faster",
      "B": "It treats sparse (categorical) and dense (numerical) features differently through separate towers",
      "C": "It's simpler to implement",
      "D": "It uses less data"
    },
    "answer": "B",
    "explanation": "MLPs simply concatenate all features, but DLRM uses separate internal 'towers' for sparse and dense features before fusion. This handles the heterogeneity better - categorical embeddings (creator_id) and numerical features (view_count) have different characteristics."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What advantage do transformers provide for recommendation ranking over DLRM?",
    "options": {
      "A": "They're always faster",
      "B": "They can model sequences with temporal order and capture complex interactions between items in history",
      "C": "They use less memory",
      "D": "They don't need training data"
    },
    "answer": "B",
    "explanation": "DLRM treats features as a bag with no temporal ordering. Transformers can model user history as a sequence, attending to interactions between items. This captures patterns like 'users who watch A then B tend to like C' that bag-of-features approaches miss."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is the role of the re-ranking layer in recommendation systems?",
    "options": {
      "A": "To re-run the ranking model",
      "B": "To apply the value model, ensure diversity, and handle business rules like creator promotion",
      "C": "To compress the results",
      "D": "To store the rankings"
    },
    "answer": "B",
    "explanation": "Re-ranking optimizes for the overall slate by: applying the value model to balance engagement/creator success/platform health, ensuring diversity, handling special cases (new creator promotion, viral content), and avoiding showing similar videos consecutively."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why might explicit user 'interests' be less useful than implicit behavioral signals?",
    "options": {
      "A": "Users don't have interests",
      "B": "Users often don't accurately know what they enjoy - revealed preferences differ from stated preferences",
      "C": "Explicit interests are private",
      "D": "They're harder to collect"
    },
    "answer": "B",
    "explanation": "Users often don't have a solid handle on what they truly enjoy - there's a gap between explicit and 'revealed' preferences. Behavioral signals (what they actually watch and engage with) are more reliable than what they say they want."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What signals indicate user dissatisfaction that might inform negative training examples?",
    "options": {
      "A": "Watching more videos",
      "B": "Leaving the platform after watching a video (user attrition signal)",
      "C": "Sharing videos",
      "D": "Subscribing to channels"
    },
    "answer": "B",
    "explanation": "Users who leave the platform after watching a video ('nope I'm done') provide strong implicit negative signals. This is more informative than going to another video (might just be browsing). These goldmine signals help train what NOT to recommend."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why is matrix factorization or collaborative filtering considered outdated for modern recommendation systems?",
    "options": {
      "A": "They're too accurate",
      "B": "Most important applications have graduated to more sophisticated approaches like transformers and deep learning",
      "C": "They're too fast",
      "D": "They don't exist anymore"
    },
    "answer": "B",
    "explanation": "While matrix factorization and collaborative filtering appear in much online material, they're not state-of-the-art anymore. Modern applications at scale use deep learning approaches like DLRM or transformers that can model complex feature interactions and sequences."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is 'hard negative sampling' in training two-tower embedding models?",
    "options": {
      "A": "Sampling difficult users",
      "B": "Biasing negative samples toward videos the user might like but didn't - near misses are more informative",
      "C": "Removing negative samples",
      "D": "Using only negative samples"
    },
    "answer": "B",
    "explanation": "Random negatives are often obviously dissimilar. Hard negative sampling biases toward videos we'd expect the user to like but they didn't engage with. These 'near misses' provide more informative training signal than random negatives."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What question types should you ask during problem clarification in an ML interview?",
    "options": {
      "A": "Questions about the interviewer's background",
      "B": "Questions about users, pain points, scale, latency requirements, and existing solutions",
      "C": "Questions about salary and benefits",
      "D": "Questions about office location"
    },
    "answer": "B",
    "explanation": "Start by asking targeted questions about: who the users are, what their pain points are, current solutions, scale requirements (users, requests per second), whether inference is real-time or batch, and specific constraints like latency or privacy."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What is a 'staff-level' behavior during problem clarification?",
    "options": {
      "A": "Asking many generic questions",
      "B": "Immediately recognizing the core challenge and probing around things teams will work on for years",
      "C": "Skipping clarification entirely",
      "D": "Only asking about the model architecture"
    },
    "answer": "B",
    "explanation": "Strong staff-level candidates quickly identify what makes a problem interesting or challenging and start probing around those areas. They demonstrate they could lead a team working on this problem for years, not just solve it once."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is MRR (Mean Reciprocal Rank) sensitive to in search/recommendation evaluation?",
    "options": {
      "A": "Total number of results",
      "B": "The rank of the very first relevant item",
      "C": "The last relevant item",
      "D": "The number of queries"
    },
    "answer": "B",
    "explanation": "MRR is sensitive to the position of the first relevant item - it's the mean of 1/rank of first relevant result. Great for 'top pick' use-cases where you mainly care about whether the best result appears first."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is 'query ambiguity' and how can it be addressed in search evaluation?",
    "options": {
      "A": "Users make typos; use spell correction",
      "B": "Many queries have multiple interpretations; use intent classification and result diversification",
      "C": "Queries are too long; truncate them",
      "D": "Users don't search; send push notifications"
    },
    "answer": "B",
    "explanation": "Queries like 'jaguar' could mean the animal, car, or sports team. Address this with intent classification systems, diversification strategies that cover multiple intents, and analyzing click patterns to understand intent distribution."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "Why is coverage an important metric for recommender systems?",
    "options": {
      "A": "It measures code coverage",
      "B": "It measures what proportion of the catalog is surfaced - critical for cold-start and long-tail content",
      "C": "It measures server coverage",
      "D": "It measures geographic coverage"
    },
    "answer": "B",
    "explanation": "Coverage measures what proportion of the catalog is shown over time. It's critical for cold-start sellers and long-tail content - if the system only recommends popular items, new content never gets discovered and the system becomes stale."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What does 'calibration' mean for a content moderation classifier?",
    "options": {
      "A": "Adjusting the screen brightness",
      "B": "Ensuring the model's output scores represent true probabilities of being harmful",
      "C": "Measuring model size",
      "D": "Setting the threshold to 0.5"
    },
    "answer": "B",
    "explanation": "A calibrated classifier's scores represent true probabilities - if the model says 95% confidence, it should be correct 95% of the time. This is essential when precision guardrails (e.g., 95% precision for auto-delete) are part of the business objective."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why are creator features useful for harmful content detection?",
    "options": {
      "A": "Creators always post harmful content",
      "B": "User history and patterns indicate likelihood of posting harmful content, even before examining content",
      "C": "Creators have more followers",
      "D": "Creator features are easier to compute"
    },
    "answer": "B",
    "explanation": "Some users are more/less likely to author harmful content based on their history. User embeddings capture profile richness (grandma in cat group vs. teenager in death videos group), and real-time tallies capture recent behavior changes."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why might appeal outcomes be valuable training data for bot detection?",
    "options": {
      "A": "Appeals are automated",
      "B": "Successful appeals provide high-confidence negative labels (accounts incorrectly flagged as bots)",
      "C": "Appeals increase revenue",
      "D": "Appeals are faster to process"
    },
    "answer": "B",
    "explanation": "When restricted accounts successfully appeal, we get high-confidence negative labels - these were legitimate users we incorrectly flagged. This helps calibrate the model and reduce false positives."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What temporal features help distinguish bots from human users?",
    "options": {
      "A": "Time zone",
      "B": "Activity patterns like circadian rhythms, burst detection, and click timing precision",
      "C": "Account creation date only",
      "D": "Calendar holidays"
    },
    "answer": "B",
    "explanation": "Humans show clear daily patterns (circadian rhythms), natural variance in click timing, and realistic error rates (typos). Bots often have too-precise timing, inhuman activity bursts, and lack natural sleep gaps. These features resist adversarial adaptation."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is a 'value model' in recommendation systems?",
    "options": {
      "A": "A model that predicts video value",
      "B": "A layer that combines multiple prediction outputs to balance engagement, creator success, and platform health",
      "C": "A model that predicts user value",
      "D": "A pricing model"
    },
    "answer": "B",
    "explanation": "The value model in re-ranking combines outputs from multiple prediction heads (watch time, CTR, like probability, etc.) to optimize for overall objectives including user engagement, creator sustainability, and platform health - not just any single metric."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "Why is the current video the user is watching important context for 'up next' recommendations?",
    "options": {
      "A": "It's easier to process",
      "B": "The current video signals user intent and interests in this session - recommendations should be contextually relevant",
      "C": "It's required by the algorithm",
      "D": "It reduces latency"
    },
    "answer": "B",
    "explanation": "The current video is a strong signal of what the user wants to watch next. A user watching a cooking tutorial probably wants more cooking content, not random popular videos. Context makes recommendations immediately relevant."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What is the risk of not mentioning research from the last 5-7 years in model selection?",
    "options": {
      "A": "No risk at all",
      "B": "Interviewers may see your knowledge as dated",
      "C": "Old models are always better",
      "D": "Research doesn't matter in interviews"
    },
    "answer": "B",
    "explanation": "While you don't need cutting-edge NeurIPS papers, missing research from the last 5-7 years risks appearing dated. Interviewers want to see awareness of modern approaches (transformers, DLRM, etc.) while also valuing practical approaches that have stood the test of time."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "Why should you discuss both qualitative and quantitative evaluation approaches?",
    "options": {
      "A": "To use more time",
      "B": "Quantitative metrics don't always correlate with perceptual quality; user studies capture what matters to humans",
      "C": "Qualitative is always better",
      "D": "Quantitative is always better"
    },
    "answer": "B",
    "explanation": "Metrics like vertex error might score poorly for small misalignments that look fine to humans, or vice versa. Combining quantitative benchmarks with user studies trades evaluation complexity for comprehensive validation that captures both accuracy and perceptual quality."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is the 'long-tail' challenge in search evaluation?",
    "options": {
      "A": "Queries are too long",
      "B": "Most queries are unique or rare, making it impractical to collect relevance judgments for every query",
      "C": "Results load slowly",
      "D": "Users have long attention spans"
    },
    "answer": "B",
    "explanation": "The vast majority of search queries are unique or very rare (the 'long tail'). Active learning, query clustering, synthetic query generation, and transfer learning from head queries help address evaluation of tail queries."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "What is the advantage of multi-modal transformer architectures for content moderation?",
    "options": {
      "A": "They're simpler to implement",
      "B": "True multi-modal learning with attention between text, images, and features catches subtle cross-modal harmful patterns",
      "C": "They use less compute",
      "D": "They only need text"
    },
    "answer": "B",
    "explanation": "Multi-modal transformers can process text, images, and tabular features simultaneously with attention between all modalities. This catches harmful content where context matters - an image that's benign alone but harmful with certain text."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "What is the benefit of using cross-attention between graph and sequence embeddings in bot detection?",
    "options": {
      "A": "It's faster",
      "B": "It lets the timeline query the graph ('does anyone in my cluster post with this rhythm?') and vice-versa",
      "C": "It uses less memory",
      "D": "It's simpler"
    },
    "answer": "B",
    "explanation": "Cross-attention between the graph (social network structure) and sequence (temporal behavior) branches enables queries like 'does anyone else in my cluster post with this rhythm?' This identifies coordinated bot networks that share both structural and behavioral patterns."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What contextual features help personalize recommendations beyond user history?",
    "options": {
      "A": "Only watch history",
      "B": "Time of day, device type, previous searches, and current session behavior",
      "C": "Only demographics",
      "D": "Only location"
    },
    "answer": "B",
    "explanation": "Context significantly impacts relevance: users might want short videos on mobile during commute but longer content on TV at night. Session context (recent searches, current video), time of day, and device type all help tailor recommendations."
  },
  {
    "source": "[Hello Interview] ML System Design Framework",
    "question": "What does data drift mean and why is it a common cause of model degradation?",
    "options": {
      "A": "Data moving between servers",
      "B": "The distribution of input data changing over time, causing models trained on old data to perform poorly",
      "C": "Data being deleted",
      "D": "Data being duplicated"
    },
    "answer": "B",
    "explanation": "Data drift occurs when the distribution of input data changes over time (e.g., user behavior evolves, new content types emerge). Models trained on old distributions may perform poorly on new data. It's a common cause of gradual performance degradation."
  },
  {
    "source": "[Hello Interview] ML System Design Evaluation",
    "question": "What is counterfactual evaluation and why is it useful for recommender systems?",
    "options": {
      "A": "Evaluating imaginary models",
      "B": "Estimating what would have happened under different recommendations using importance weighting",
      "C": "Counting facts in the data",
      "D": "Evaluating counter-examples"
    },
    "answer": "B",
    "explanation": "Counterfactual evaluation estimates how a new policy (recommender) would have performed on data collected under an old policy. It uses importance weighting to adjust for the difference in what was shown. This enables offline evaluation without live A/B tests."
  },
  {
    "source": "[Hello Interview] Harmful Content Detection",
    "question": "Why might 'user satisfaction scores' or user reports have limitations as sole metrics for content moderation?",
    "options": {
      "A": "Users always give accurate feedback",
      "B": "Reports can be adversarial (disagreeing with benign content) or incorrect (unfamiliar with policies)",
      "C": "Reports are too accurate",
      "D": "Users don't report anything"
    },
    "answer": "B",
    "explanation": "User reports aren't perfect: they may be adversarial (reporting benign content because the reporter disagrees with it) or incorrect (unfamiliar with platform policies). They're valuable signals but shouldn't be the sole metric."
  },
  {
    "source": "[Hello Interview] Bot Detection",
    "question": "Why is bot detection compute often 'net positive' for platforms?",
    "options": {
      "A": "Bots generate revenue",
      "B": "The cost of detecting and preventing bot activity is often less than the compute cost of serving bot traffic",
      "C": "Bots pay for compute",
      "D": "Detection is free"
    },
    "answer": "B",
    "explanation": "Bot remediation frequently has positive net impact on compute - detecting and blocking bots consumes less resources than serving their traffic (which can be massive). However, this doesn't mean detection can be inefficient."
  },
  {
    "source": "[Hello Interview] Video Recommendation",
    "question": "What is the 'cold start' problem and how do multiple prediction heads help?",
    "options": {
      "A": "Servers being too cold",
      "B": "New items/users have no history; multi-task learning leverages correlations between metrics to make predictions",
      "C": "Videos starting to play slowly",
      "D": "Models starting training"
    },
    "answer": "B",
    "explanation": "Cold start refers to new items or users with no interaction history. Multi-task learning helps by leveraging correlations - if we can predict one signal (e.g., from content features), related signals (engagement) can be inferred through learned correlations."
  }
]
